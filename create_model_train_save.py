
import os
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from datetime import datetime
import pandas as pd

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
BASE_DIR = "data/data_training/CRYPTO"
SAVE_DIR = "modelsave"
EPOCHS = 50
BATCH_SIZE = 64
MIN_SAMPLES = 100  # –º–∏–Ω–∏–º—É–º –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤


def build_model():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã LSTM + Transformer + MLP"""
    inp = layers.Input(shape=(60, 5))

    # === LSTM –±–ª–æ–∫ ===
    x = layers.LSTM(60, activation='tanh', recurrent_activation='sigmoid', return_sequences=True)(inp)
    x = layers.LSTM(60, activation='tanh', return_sequences=False)(x)
    x = layers.Reshape((60, 1))(x)
    x = layers.Dropout(0.1)(x)
    lstm_out = x

    # === Transformer Encoder ===
    norm1 = layers.LayerNormalization(epsilon=1e-6)(lstm_out)
    attn_out = layers.MultiHeadAttention(num_heads=5, key_dim=120, dropout=0.15)(norm1, norm1)
    attn_out = layers.Dropout(0.15)(attn_out)
    res1 = layers.Add()([attn_out, lstm_out])

    norm2 = layers.LayerNormalization(epsilon=1e-6)(res1)
    dense1 = layers.Dense(5, activation='relu')(norm2)
    dense1 = layers.Dropout(0.15)(dense1)
    dense2 = layers.Dense(60, activation='linear')(dense1)
    res2 = layers.Add()([dense2, res1])

    # === MLP Decoder ===
    x = layers.GlobalAveragePooling1D(data_format='channels_first')(res2)
    x = layers.Dropout(0.10)(x)
    x = layers.Dense(30, activation='relu')(x)
    out = layers.Dense(5, activation='linear')(x)

    model = models.Model(inputs=inp, outputs=out)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])
    return model



def train_and_save_model(coin_dir, coin_name):
    """–û–±—É—á–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å (—Å —è–≤–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π)"""
    x_path = os.path.join(coin_dir, "x_train.npy")
    y_path = os.path.join(coin_dir, "y_train.npy")

    if not os.path.exists(x_path) or not os.path.exists(y_path):
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {coin_name}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç x_train.npy –∏–ª–∏ y_train.npy")
        return

    X = np.load(x_path, allow_pickle=True)
    y = np.load(y_path, allow_pickle=True)

    if len(X) < MIN_SAMPLES:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {coin_name}: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö ({len(X)} < {MIN_SAMPLES})")
        log_path = os.path.join(SAVE_DIR, coin_name, "log_warning.txt")
        os.makedirs(os.path.join(SAVE_DIR, coin_name), exist_ok=True)
        with open(log_path, "w") as log:
            log.write(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {coin_name}\n")
            log.write(f"–†–∞–∑–º–µ—Ä X: {X.shape}, y: {y.shape}\n")
            log.write(f"–¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º: {MIN_SAMPLES}\n")
        return

    # === –î–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val ===
    split_index = int(0.8 * len(X))
    X_train, X_val = X[:split_index], X[split_index:]
    y_train, y_val = y[:split_index], y[split_index:]

    print(f"\nüß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {coin_name}")
    print(f"   X_train: {X_train.shape} | y_train: {y_train.shape}")
    print(f"   X_val:   {X_val.shape}   | y_val:   {y_val.shape}")

    model = build_model()

    # === –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ===
    time_tag = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(SAVE_DIR, coin_name)
    os.makedirs(save_path, exist_ok=True)

    # === –ö–æ–ª–ª–±—ç–∫–∏ ===
    log_dir = os.path.join(save_path, f"logs_{time_tag}.txt")
    checkpoint_path = os.path.join(save_path, f"model_{time_tag}.keras")

    csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(save_path, f"history_{time_tag}.csv"))
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1
    )

    # === –û–±—É—á–µ–Ω–∏–µ ===
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_val, y_val),
        verbose=1,
        callbacks=[csv_logger, checkpoint]
    )

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å ===
    final_model_path = os.path.join(save_path, f"final_model_{time_tag}.keras")
    model.save(final_model_path)

    # === –õ–æ–≥ ===
    with open(log_dir, "w") as log:
        log.write(f"üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {coin_name}\n")
        log.write(f"–í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now()}\n")
        log.write(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\n")
        log.write(f"  X_train: {X_train.shape}, y_train: {y_train.shape}\n")
        log.write(f"  X_val:   {X_val.shape}, y_val:   {y_val.shape}\n")
        log.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {EPOCHS}\n")
        log.write(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}\n")
        log.write(f"–õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}\n")
        log.write("\nüìä –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è:\n")
        df = pd.DataFrame(history.history)
        log.write(df.tail().to_string())

    print(f"‚úÖ –ú–æ–¥–µ–ª—å {coin_name} –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_path}")

# === –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª ===
if __name__ == "__main__":
    for coin_name in os.listdir(BASE_DIR):
        coin_dir = os.path.join(BASE_DIR, coin_name)
        if os.path.isdir(coin_dir):
            train_and_save_model(coin_dir, coin_name)

    print("\nüéØ –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")

